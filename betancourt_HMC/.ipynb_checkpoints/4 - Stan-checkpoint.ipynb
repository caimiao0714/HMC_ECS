{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pystan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36e134289f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpystan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstan_utility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pystan'"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Copyright 2019 Michael Betancourt\n",
    "# Licensed under the new BSD (3-clause) license:\n",
    "#\n",
    "# https://opensource.org/licenses/BSD-3-Clause\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "# Initial setup\n",
    "############################################################\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "import pystan\n",
    "import stan_utility\n",
    "\n",
    "help(stan_utility)\n",
    "\n",
    "light=\"#DCBCBC\"\n",
    "light_highlight=\"#C79999\"\n",
    "mid=\"#B97C7C\"\n",
    "mid_highlight=\"#A25050\"\n",
    "dark=\"#8F2727\"\n",
    "dark_highlight=\"#7C0000\"\n",
    "green=\"#00FF00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the computation of Markov chain Monte Carlo estimators, let's define a _Welford accumulator_ that computes empirical summaries of a sample in a single pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welford_summary(x, L = 100):\n",
    "  summary = [0] * (L + 1)\n",
    "  for n in range(len(x)):\n",
    "    delta = x[n] - summary[0]\n",
    "    summary[0] += delta / (n + 1)\n",
    "    for l in range(L):\n",
    "      if n > l:\n",
    "        summary[l + 1] += delta * (x[n - l] - summary[0])\n",
    "\n",
    "  norm = 1.0 / (len(x) - 1)\n",
    "  for l in range(L): summary[l + 1] *= norm\n",
    "  return summary\n",
    "\n",
    "# We can then use the Welford accumulator output to compute the\n",
    "# Markov chain Monte Carlo estimators and their properties\n",
    "def compute_mcmc_stats(x, L = 20):\n",
    "  summary = welford_summary(x, L)\n",
    "  \n",
    "  mean = summary[0]\n",
    "  var = summary[1]\n",
    "  acov = summary[1:(L + 1)]\n",
    "  \n",
    "  # Compute the effective sample size\n",
    "  rho_hat_s = [0] * L\n",
    "  rho_hat_s[1] = acov[1] / var\n",
    "  \n",
    "  # First we transform our autocovariances into Geyer's initial positive sequence\n",
    "  max_s = 1\n",
    "  for s in [ 2 * i + 1 for i in range((L - 1) / 2) ]:\n",
    "    rho_hat_even = acov[s + 1] / var\n",
    "    rho_hat_odd = acov[s + 2] / var;\n",
    "    \n",
    "    max_s = s + 2  \n",
    "    \n",
    "    if rho_hat_even + rho_hat_odd > 0:\n",
    "      rho_hat_s[s + 1] = rho_hat_even\n",
    "      rho_hat_s[s + 2] = rho_hat_odd\n",
    "    else:   \n",
    "      break\n",
    "  \n",
    "  # Then we transform this output into Geyer's initial monotone sequence\n",
    "  for s in [ 2 * i + 3 for i in range((max_s - 2)/ 2) ]:\n",
    "    if rho_hat_s[s + 1] + rho_hat_s[s + 2] > rho_hat_s[s - 1] + rho_hat_s[s]:\n",
    "      rho_hat_s[s + 1] = 0.5 * (rho_hat_s[s - 1] + rho_hat_s[s])\n",
    "      rho_hat_s[s + 2] = rho_hat_s[s + 1]\n",
    "  \n",
    "  ess = len(x) / (1.0 + 2 * sum(rho_hat_s))\n",
    "  \n",
    "  return [mean, math.sqrt(var / ess), math.sqrt(var), ess]\n",
    "\n",
    "def compute_running_estimator(x):\n",
    "  N = len(x)\n",
    "  stride = 50\n",
    "  M = N / stride\n",
    "\n",
    "  iters = [ stride * (i + 1) for i in range(M) ]\n",
    "  \n",
    "  x1_mean = [0] * M \n",
    "  x1_se = [0] * M\n",
    "\n",
    "  for m in range(M):\n",
    "    running_samples = x0[0:iters[m]]\n",
    "    mcmc_stats = compute_mcmc_stats(running_samples)\n",
    "    x1_mean[m] = mcmc_stats[0]\n",
    "    x1_se[m] = mcmc_stats[1]\n",
    "    \n",
    "  return iters, x1_mean, x1_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Stan program and fit with dynamic Hamiltonian Monte Carlo\n",
    "model = stan_utility.compile_model('normal.stan')\n",
    "fit = model.sampling(seed=4838282)\n",
    "\n",
    "# Check diagnostics\n",
    "stan_utility.check_all_diagnostics(fit)\n",
    "\n",
    "# Check MCMC estimators\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-t Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = stan_utility.compile_model('student_t.stan')\n",
    "\n",
    "# 100 degrees of freedom\n",
    "data = dict(nu = 100)\n",
    "fit100 = model.sampling(data=data, seed=4838282,\n",
    "                        control=dict(metric=\"unit_e\", stepsize=0.7, adapt_engaged=False))\n",
    "stan_utility.check_all_diagnostics(fit100)\n",
    "\n",
    "sampler_params = fit100.get_sampler_params(inc_warmup=False)\n",
    "stepsizes100 = [sampler_params[n]['stepsize__'][0] \n",
    "                for n in range(4) ]\n",
    "times100 = [stepsizes100[n] * x for n in range(4)\n",
    "          for x in sampler_params[n]['n_leapfrog__'] ]\n",
    "energies100 = [x for y in sampler_params for x in y['energy__']]\n",
    "\n",
    "x0 = fit100.extract(permuted=False)[:,:,0].flatten()\n",
    "iters, x1_mean, x1_se = compute_running_estimator(x0)\n",
    "\n",
    "plot.fill_between(iters, \n",
    "                  [ x1_mean[m] - 2 * x1_se[m] for m in range(len(iters)) ],\n",
    "                  [ x1_mean[m] + 2 * x1_se[m] for m in range(len(iters)) ],\n",
    "                  facecolor=light, color=light)\n",
    "plot.plot(iters, x1_mean, color=dark)\n",
    "plot.plot([iters[0], iters[-1]], [0, 0], color='grey', linestyle='--')\n",
    "\n",
    "plot.gca().set_xlim([0, 4000])\n",
    "plot.gca().set_xlabel(\"Iteration\")\n",
    "plot.gca().set_ylim([-0.5, 0.5])\n",
    "plot.gca().set_ylabel(\"Monte Carlo Estimator\")\n",
    "\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 degrees of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(nu = 5)\n",
    "fit5 = model.sampling(data=data, seed=4838282, \n",
    "                      control=dict(metric=\"unit_e\", stepsize=0.7, adapt_engaged=False))\n",
    "stan_utility.check_all_diagnostics(fit5)\n",
    "\n",
    "sampler_params = fit5.get_sampler_params(inc_warmup=False)\n",
    "stepsizes5 = [sampler_params[n]['stepsize__'][0] \n",
    "              for n in range(4) ]\n",
    "times5 = [stepsizes5[n] * x for n in range(4)\n",
    "          for x in sampler_params[n]['n_leapfrog__'] ]\n",
    "energies5 = [x for y in sampler_params for x in y['energy__']]\n",
    "\n",
    "# 2 degrees of freedom -- no more component variances!\n",
    "data = dict(nu = 2)\n",
    "fit2 = model.sampling(data=data, seed=4838282,\n",
    "                      control=dict(metric=\"unit_e\", stepsize=0.7, adapt_engaged=False))\n",
    "stan_utility.check_all_diagnostics(fit2)\n",
    "\n",
    "sampler_params = fit2.get_sampler_params(inc_warmup=False)\n",
    "stepsizes2 = [sampler_params[n]['stepsize__'][0] \n",
    "              for n in range(4) ]\n",
    "times2 = [stepsizes2[n] * x for n in range(4)\n",
    "          for x in sampler_params[n]['n_leapfrog__'] ]\n",
    "energies2 = [x for y in sampler_params for x in y['energy__']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 degree of freedom -- no more component means or variances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(nu = 1)\n",
    "fit1 = model.sampling(data=data, seed=4838282,\n",
    "                      control=dict(metric=\"unit_e\", stepsize=0.7, \n",
    "                                   max_treedepth=12, adapt_engaged=False))\n",
    "stan_utility.check_all_diagnostics(fit1, 12)\n",
    "\n",
    "sampler_params = fit1.get_sampler_params(inc_warmup=False)\n",
    "stepsizes1 = [sampler_params[n]['stepsize__'][0] \n",
    "              for n in range(4) ]\n",
    "times1 = [stepsizes1[n] * x for n in range(4)\n",
    "          for x in sampler_params[n]['n_leapfrog__'] ]\n",
    "energies1 = [x for y in sampler_params for x in y['energy__']]\n",
    "\n",
    "x0 = fit1.extract(permuted=False)[:,:,0].flatten()\n",
    "iters, x1_mean, x1_se = compute_running_estimator(x0)\n",
    "\n",
    "plot.fill_between(iters, \n",
    "                  [ x1_mean[m] - 2 * x1_se[m] for m in range(len(iters)) ],\n",
    "                  [ x1_mean[m] + 2 * x1_se[m] for m in range(len(iters)) ],\n",
    "                  facecolor=light, color=light)\n",
    "plot.plot(iters, x1_mean, color=dark)\n",
    "plot.plot([iters[0], iters[-1]], [0, 0], color='grey', linestyle='--')\n",
    "\n",
    "plot.gca().set_xlim([0, 4000])\n",
    "plot.gca().set_xlabel(\"Iteration\")\n",
    "plot.gca().set_ylim([-50, 50])\n",
    "plot.gca().set_ylabel(\"Monte Carlo Estimator\")\n",
    "\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot comparison of integration time scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plot.subplots(1, 4)\n",
    "    \n",
    "axarr[0].set_title(\"nu = 100\")\n",
    "axarr[0].scatter(energies100, [math.log(x) for x in times100], color=dark, alpha=0.1)\n",
    "axarr[0].set_xlim(0, 50)\n",
    "axarr[0].set_xlabel(\"Energy\")\n",
    "axarr[0].set_ylim([0, 7])\n",
    "axarr[0].set_ylabel(\"Log Integration Time\")\n",
    "\n",
    "axarr[1].set_title(\"nu = 5\")\n",
    "axarr[1].scatter(energies5, [math.log(x) for x in times5], color=dark, alpha=0.1)\n",
    "axarr[1].set_xlim(0, 50)\n",
    "axarr[1].set_xlabel(\"Energy\")\n",
    "axarr[1].set_ylim([0, 7])\n",
    "\n",
    "axarr[2].set_title(\"nu = 2\")\n",
    "axarr[2].scatter(energies2, [math.log(x) for x in times2], color=dark, alpha=0.1)\n",
    "axarr[2].set_xlim(0, 50)\n",
    "axarr[2].set_xlabel(\"Energy\")\n",
    "axarr[2].set_ylim([0, 7])\n",
    "\n",
    "axarr[3].set_title(\"nu = 1\")\n",
    "axarr[3].scatter(energies1, [math.log(x) for x in times1], color=dark, alpha=0.1)\n",
    "axarr[3].set_xlim(0, 50)\n",
    "axarr[3].set_xlabel(\"Energy\")\n",
    "axarr[3].set_ylim([0, 7])\n",
    "\n",
    "plot.subplots_adjust(wspace=0.5)\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "Let's consider an example in the context of Bayesian inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first simulate an observation and save it to a file\n",
    "N = 1\n",
    "simu_data = dict(N = N)\n",
    "\n",
    "simu_model = stan_utility.compile_model('simulate_data.stan')\n",
    "simu = simu_model.sampling(data=simu_data, iter=1, chains=1, seed=4838282,\n",
    "                           algorithm=\"Fixed_param\")\n",
    "\n",
    "data = dict(N = N, y = simu.extract()['y'].flatten())\n",
    "pystan.stan_rdump(data, 'simulation.data.R')\n",
    "\n",
    "# Now we can read that data back in and use Hamiltonian\n",
    "# Monte Carlo to estimate posterior expectation values\n",
    "input_data = pystan.read_rdump('simulation.data.R')\n",
    "\n",
    "model = stan_utility.compile_model('fit_data.stan')\n",
    "fit = model.sampling(data=input_data, seed=4938483)\n",
    "\n",
    "# Check diagnostics\n",
    "stan_utility.check_all_diagnostics(fit)\n",
    "\n",
    "# That doesn't look good.  Let's investigate the divergent\n",
    "# samples in the context of the non-divergent samples to\n",
    "# see what's going on.\n",
    "nondiv_params, div_params = stan_utility.partition_div(fit)\n",
    "\n",
    "plot.scatter(nondiv_params['mu'],\n",
    "             [math.log(x) for x in nondiv_params['sigma']],\n",
    "              color = mid_highlight, alpha=0.05)\n",
    "plot.scatter(div_params['mu'],\n",
    "             [math.log(x) for x in div_params['sigma']],\n",
    "              color = green, alpha=0.5)\n",
    "\n",
    "plot.gca().set_xlabel(\"mu\")\n",
    "plot.gca().set_ylabel(\"sigma\")\n",
    "plot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
